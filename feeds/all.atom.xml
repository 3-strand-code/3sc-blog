<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>3 Strand Code Blog</title><link href="http://3-strand-code.github.io/3sc-blog/" rel="alternate"></link><link href="http://3-strand-code.github.io/3sc-blog/feeds/all.atom.xml" rel="self"></link><id>http://3-strand-code.github.io/3sc-blog/</id><updated>2015-11-20T10:20:00-08:00</updated><entry><title>Neural Networks Part 1: The Neuron</title><link href="http://3-strand-code.github.io/3sc-blog/neural-networks-part-1-the-neuron.html" rel="alternate"></link><updated>2015-11-20T10:20:00-08:00</updated><author><name>Eric Carmichael</name></author><id>tag:3-strand-code.github.io,2015-11-20:3sc-blog/neural-networks-part-1-the-neuron.html</id><summary type="html">&lt;p&gt;Last month we did the first part of a many part series about creating Neural Networks
from the ground up.  See it on &lt;a href="https://github.com/dev-coop/neural-net-hacking-examples"&gt;GitHub&lt;/a&gt;.
I'll briefly cover some of what Levi went over but I won't do it justice! 
You'll have to come to the meet-ups to get the full effect.&lt;/p&gt;
&lt;p&gt;Basically, neural networks can be used to take in some input and produce some desired output.
Like, taking an image as input and &lt;a href="http://www.digitaltrends.com/computing/realtime-neural-network-amsterdam/"&gt;outputting a caption&lt;/a&gt; for the image.&lt;/p&gt;
&lt;p&gt;The first session covered the most basic piece of a neural network: the neuron. Full
neural networks consist of a few more parts, but they are all there to glue together
the basic neuron.&lt;/p&gt;
&lt;h3&gt;Neuron Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;takes any number of inputs&lt;/li&gt;
&lt;li&gt;each input has a connection weight multiplier&lt;/li&gt;
&lt;li&gt;input values are multiplied by their connection weight and summed&lt;/li&gt;
&lt;li&gt;the multiplied sum total is passed through an activation function to produce the output&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Neuron Model" src="images/neuron-model.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;&lt;a href="https://en.wikibooks.org/wiki/Artificial_Neural_Networks/Activation_Functions"&gt;Source: WikiBooks&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h3&gt;Artificial Neurons&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;connections to other neurons via memory pointers&lt;/li&gt;
&lt;li&gt;fires by calling a method on the Neuron object&lt;/li&gt;
&lt;li&gt;digital&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Real Neurons&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;branching dendrites to receive chemical signals&lt;/li&gt;
&lt;li&gt;axon projection to conduct a nerve signal&lt;/li&gt;
&lt;li&gt;analog&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Neuron" src="images/neuron.png" /&gt;&lt;/p&gt;
&lt;p&gt;We started by making one neuron and activating it. Activation functions
introduce non-linearity into the network, we'll why that is important when we get to training.
Normalizing our inputs from 0...1 (the range of our activation function) makes it more effective.&lt;/p&gt;
&lt;p&gt;We ended up using the &lt;a href="http://www.wikiwand.com/en/Sigmoid_function"&gt;Sigmoid&lt;/a&gt; activation function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After completing the activation function we connected the neurons to each other.&lt;/p&gt;
&lt;h3&gt;Example neuron code&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/dev-coop/neural-net-hacking-examples/blob/master/elixir/neuron.exs"&gt;elixir&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dev-coop/neural-net-hacking-examples/blob/master/es5/neuronet.js"&gt;javascript ES5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dev-coop/neural-net-hacking-examples/blob/master/es7/Neuron.js"&gt;javascript ES7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dev-coop/neural-net-hacking-examples/blob/master/ruby/karmen_neural_network.rb"&gt;ruby&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dev-coop/neural-net-hacking-examples/blob/master/python/neural_network_with_connections.py"&gt;python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dev-coop/neural-net-hacking-examples/blob/master/python/neural_network_with_connections_tests.py"&gt;python tests&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Stay tuned for part 2 next month! We'll be layering and maybe even training our neurons!&lt;/p&gt;
&lt;h3&gt;Bonus: Python tests&lt;/h3&gt;
&lt;p&gt;In Python a tests are many functions run one after the other checking a suite of software for errors.
These functions have simple assertions in them guaranteeing the behavior of a piece of code.
If the assertion fails, the test fails.&lt;/p&gt;
&lt;p&gt;The assertion is a condition that evaluates to a boolean, like "assert that when I add 1 + 1 I get the result 2."&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_add_one_plus_one_equals_two&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can use tests to help maintain our code and get rid of unforeseen bugs. For example, if you
are working on a website with 10,000 lines of code... how can you be sure when you touch &lt;em&gt;this thing over here&lt;/em&gt;
it doesn't break &lt;em&gt;that thing over there&lt;/em&gt;? Well! You write about twice as much code making sure the original
code does what it's supposed to by asserting exactly how it should behave.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/dev-coop/neural-net-hacking-examples/blob/master/python/neural_network_with_connections_tests.py"&gt;Here's&lt;/a&gt; a test from the Python example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_connection_adds_to_incoming_and_outgoing_arrays&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c"&gt;# Setup our neurons and connections&lt;/span&gt;
    &lt;span class="n"&gt;neuron&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Neuron&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;neuron_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Neuron&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;neuron&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;connect_child&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;neuron_2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c"&gt;# Let&amp;#39;s make sure that there&amp;#39;s at least one connection&lt;/span&gt;
    &lt;span class="c"&gt;# from our neuron to the next neuron&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;neuron&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;connection&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;neuron&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;connection&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;neuron_2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;incoming_neurons&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

    &lt;span class="c"&gt;# And the same goes for the other neuron, make sure&lt;/span&gt;
    &lt;span class="c"&gt;# it&amp;#39;s connected to us&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;neuron_2&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;connection&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;neuron&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;connection&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;neuron&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outgoing_neurons&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If the above test passes, we can be sure our connection was made properly!&lt;/p&gt;</summary><category term="neural networks"></category><category term="machine learning"></category><category term="coding"></category></entry></feed>