<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>3 Strand Code Blog</title><link href="http://blog.3strandcode.com/" rel="alternate"></link><link href="http://blog.3strandcode.com/feeds/all.atom.xml" rel="self"></link><id>http://blog.3strandcode.com/</id><updated>2016-01-06T10:20:00-08:00</updated><entry><title>Neural Networks Part 4:</title><link href="http://blog.3strandcode.com/neural-networks-part-4.html" rel="alternate"></link><updated>2016-01-06T10:20:00-08:00</updated><author><name>Eric Carmichael</name></author><id>tag:blog.3strandcode.com,2016-01-06:neural-networks-part-4.html</id><summary type="html">&lt;p&gt;This session we didn't add much new behavior, but tied things nicely together in a &lt;code&gt;Trainer&lt;/code&gt; class and added 
nicer output to track the current error.&lt;/p&gt;
&lt;p&gt;Here's an example usage of the new class:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;OR_GATE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="c"&gt;# (input, output)&lt;/span&gt;
    &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Network&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;trainer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Trainer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;OR_GATE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;When run this outputs:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;python neural_network_with_connections.py
Epoch  &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="nv"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; 0.0976866206323
Epoch  &lt;span class="m"&gt;1000&lt;/span&gt; &lt;span class="nv"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; 0.0175155899397
Epoch  &lt;span class="m"&gt;2000&lt;/span&gt; &lt;span class="nv"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; 0.00643827724377
Epoch  &lt;span class="m"&gt;3000&lt;/span&gt; &lt;span class="nv"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; 0.00322907067653
Epoch  &lt;span class="m"&gt;4000&lt;/span&gt; &lt;span class="nv"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; 0.00191442445754
Epoch  &lt;span class="m"&gt;5000&lt;/span&gt; &lt;span class="nv"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; 0.00125869287738
Epoch  &lt;span class="m"&gt;6000&lt;/span&gt; &lt;span class="nv"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; 0.000887481325948
Epoch  &lt;span class="m"&gt;7000&lt;/span&gt; &lt;span class="nv"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; 0.000657994207371
Epoch  &lt;span class="m"&gt;8000&lt;/span&gt; &lt;span class="nv"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; 0.000506639806887
Epoch  &lt;span class="m"&gt;9000&lt;/span&gt; &lt;span class="nv"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; 0.000401750116057
Epoch  &lt;span class="m"&gt;10000&lt;/span&gt; &lt;span class="nv"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; 0.000326163968267
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice as we train the &lt;code&gt;error&lt;/code&gt; gets lower and lower, our neural network is learning!&lt;/p&gt;
&lt;p&gt;Such cool stuff, come checkout the &lt;a href="http://www.meetup.com/dev-coop/"&gt;Dev Coop meet up&lt;/a&gt; in Liberty Lake, WA!&lt;/p&gt;</summary><category term="neural networks"></category><category term="machine learning"></category><category term="coding"></category></entry><entry><title>Presenting at Shaw Middle School</title><link href="http://blog.3strandcode.com/presenting-at-shaw-middle-school.html" rel="alternate"></link><updated>2016-01-01T10:20:00-08:00</updated><author><name>Eric Carmichael</name></author><id>tag:blog.3strandcode.com,2016-01-01:presenting-at-shaw-middle-school.html</id><summary type="html">&lt;p&gt;&lt;img alt="On Track Academy" src="/images/events/ontrack_academy_2.jpg" /&gt; We get the opportunity to talk to students in our community every so often, this time we're talking to a group of students
in the &lt;a href="http://www.spokaneschools.org/ontrack"&gt;On Track Academy&lt;/a&gt; at &lt;a href="http://www.spokaneschools.org/Domain/1179"&gt;Shaw Middle School&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;They were a great group of enthusiastic students... in fact, one bright student is already doing animation and experimenting 
with making her own webpages! They asked lots of questions, got excited, and applied for the program. Can't wait to see
where we are a year from now!&lt;/p&gt;
&lt;p&gt;&lt;img alt="On Track Academy" src="/images/events/ontrack_academy_1.jpg" /&gt;
&lt;img alt="On Track Academy" src="/images/events/ontrack_academy_3.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;Thanks to &lt;a href="https://www.linkedin.com/in/williamtualaufale"&gt;Tualau Fale&lt;/a&gt; for inviting us! He's a great guy and we hope to
hang out again soon!&lt;/p&gt;</summary><category term="presenting"></category><category term="community"></category><category term="coding"></category></entry><entry><title>Neural Networks Part 3: Training</title><link href="http://blog.3strandcode.com/neural-networks-part-3-training.html" rel="alternate"></link><updated>2015-12-24T10:20:00-08:00</updated><author><name>Eric Carmichael</name></author><id>tag:blog.3strandcode.com,2015-12-24:neural-networks-part-3-training.html</id><summary type="html">&lt;p&gt;We took our neurons where previously we had calculated their values and weights, this week
we added errors and real training!&lt;/p&gt;
&lt;p&gt;We finally got an actual neural network trained to output '1' when input '2'! I know that doesn't
sound very cool, but building it from scratch with great friends was super awesome.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Neuron graph" src="/images/events/neuron_pt3_remote.png" /&gt;&lt;/p&gt;
&lt;p&gt;Thanks for attending guys, have a great Christmas/New Years!&lt;/p&gt;</summary><category term="neural networks"></category><category term="machine learning"></category><category term="coding"></category></entry><entry><title>Presenting at K-Tec</title><link href="http://blog.3strandcode.com/presenting-at-k-tec.html" rel="alternate"></link><updated>2015-12-18T10:20:00-08:00</updated><author><name>Eric Carmichael</name></author><id>tag:blog.3strandcode.com,2015-12-18:presenting-at-k-tec.html</id><summary type="html">&lt;p&gt;&lt;img alt="KTec" src="/images/events/ktec.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;A few weeks ago we went to &lt;a href="http://ktectraining.org/"&gt;ktec&lt;/a&gt;, an awesome technical school in Rathdrum, Idaho. We did a
little presentation about our lives (far from perfect), how we got here (successful developers), and what we're doing to help others
achieve the same ends (&lt;a href="http://3strandcode.com"&gt;3 Strand Code!&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;We first met ktec's computer hardware instructor, &lt;a href="http://ktectraining.org/jason-green.html"&gt;Jason Green&lt;/a&gt; a couple
months ago.  He was very open and excited about what we're offering, high tech job training and placement.
He introduced us to his students and we were blown away by their response.
We're super thankful to be collaborating with Jason and ktec.&lt;/p&gt;
&lt;p&gt;We heard comments like: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[3 Strand Code] is exactly what I'm looking for... it's like you guys are giving me everything on
a turkey platter!&lt;/p&gt;
&lt;p&gt;&amp;mdash; Thai&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Game hacking&lt;/h3&gt;
&lt;p&gt;&lt;img src="/images/events/age_of_empires.png" alt="KTec" style="float: left; left: 0; transform: none; margin: 1em;" width="300"&gt;&lt;/p&gt;
&lt;p&gt;Last week Jason and I did some &lt;a href="http://autoitscript.com"&gt;AutoIt&lt;/a&gt; experiments using the aging classic 
&lt;a href="http://www.ageofempires.com/"&gt;Age of Empires II&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Jason got AutoIt to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Iterate through his barracks to build as many spearmen as possible&lt;/li&gt;
&lt;li&gt;Send one unit to explore whole map using a waypoint system&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Meanwhile many students were exploring HTML and the &lt;a href="http://getbootstrap.com"&gt;Bootstrap&lt;/a&gt; framework. A few other students were
doing some advanced work with 3d modeling and unity.&lt;/p&gt;
&lt;p&gt;Rock on Ktec, we hope to be back soon!&lt;/p&gt;</summary><category term="presenting"></category><category term="community"></category><category term="coding"></category></entry><entry><title>Neural Networks Part 2: The Network</title><link href="http://blog.3strandcode.com/neural-networks-part-2-the-network.html" rel="alternate"></link><updated>2015-12-16T10:20:00-08:00</updated><author><name>Eric Carmichael</name></author><id>tag:blog.3strandcode.com,2015-12-16:neural-networks-part-2-the-network.html</id><summary type="html">&lt;p&gt;Every month we participate in the &lt;a href="http://meetup.com/dev-coop/"&gt;Developer Co-op&lt;/a&gt; meetup in Liberty Lake, Washington. This month we
had part 2 of learning neural networks. We talked projects, ate pizza, and had a great time!&lt;/p&gt;
&lt;h3&gt;Layers and networks&lt;/h3&gt;
&lt;p&gt;This week &lt;a href="https://www.linkedin.com/in/levithomason"&gt;Levi Thomason&lt;/a&gt; went over his next lesson on neural networks. We made layers,
connected them to each other, and then connected them all together in a network.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Neuron graph" src="/images/events/neuron_pt2_graph_tree.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;In preparation for this meet up &lt;a href="https://github.com/jason-rutherford"&gt;Jason Rutherford&lt;/a&gt; made an awesome &lt;a href="https://github.com/jason-rutherford/neural-network"&gt;Ruby implementation&lt;/a&gt; of his neural network
including tests an award winning readme! :)&lt;/p&gt;
&lt;p&gt;&lt;img alt="Lookin' good boys!" src="/images/events/neuron_pt2_lookin_good_boys.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;Lookin good boys! &lt;/p&gt;
&lt;h3&gt;What's next&lt;/h3&gt;
&lt;p&gt;We'll enable our networks to be trained to output desired values based on some input.&lt;/p&gt;</summary><category term="neural networks"></category><category term="machine learning"></category><category term="coding"></category></entry><entry><title>Neural Networks Part 1: The Neuron</title><link href="http://blog.3strandcode.com/neural-networks-part-1-the-neuron.html" rel="alternate"></link><updated>2015-11-20T10:20:00-08:00</updated><author><name>Eric Carmichael</name></author><id>tag:blog.3strandcode.com,2015-11-20:neural-networks-part-1-the-neuron.html</id><summary type="html">&lt;p&gt;Last month we did the first part of a many part series about creating Neural Networks
from the ground up.  See it on &lt;a href="https://github.com/dev-coop/neural-net-hacking-examples"&gt;GitHub&lt;/a&gt;.
I'll briefly cover some of what Levi went over but I won't do it justice! 
You'll have to come to the meet-ups to get the full effect.&lt;/p&gt;
&lt;p&gt;Basically, neural networks can be used to take in some input and produce some desired output.
Like, taking an image as input and &lt;a href="http://www.digitaltrends.com/computing/realtime-neural-network-amsterdam/"&gt;outputting a caption&lt;/a&gt; for the image.&lt;/p&gt;
&lt;p&gt;The first session covered the most basic piece of a neural network: the neuron. Full
neural networks consist of a few more parts, but they are all there to glue together
the basic neuron.&lt;/p&gt;
&lt;h3&gt;Neuron Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;takes any number of inputs&lt;/li&gt;
&lt;li&gt;each input has a connection weight multiplier&lt;/li&gt;
&lt;li&gt;input values are multiplied by their connection weight and summed&lt;/li&gt;
&lt;li&gt;the multiplied sum total is passed through an activation function to produce the output&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Neuron Model" src="/images/neuron-model.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;&lt;a href="https://en.wikibooks.org/wiki/Artificial_Neural_Networks/Activation_Functions"&gt;Source: WikiBooks&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h3&gt;Artificial Neurons&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;connections to other neurons via memory pointers&lt;/li&gt;
&lt;li&gt;fires by calling a method on the Neuron object&lt;/li&gt;
&lt;li&gt;digital&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Real Neurons&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;branching dendrites to receive chemical signals&lt;/li&gt;
&lt;li&gt;axon projection to conduct a nerve signal&lt;/li&gt;
&lt;li&gt;analog&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Neuron" src="/images/neuron.png" /&gt;&lt;/p&gt;
&lt;p&gt;We started by making one neuron and activating it. Activation functions
introduce non-linearity into the network, we'll why that is important when we get to training.
Normalizing our inputs from 0...1 (the range of our activation function) makes it more effective.&lt;/p&gt;
&lt;p&gt;We ended up using the &lt;a href="http://www.wikiwand.com/en/Sigmoid_function"&gt;Sigmoid&lt;/a&gt; activation function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After completing the activation function we connected the neurons to each other.&lt;/p&gt;
&lt;h3&gt;Example neuron code&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/dev-coop/neural-net-hacking-examples/blob/master/elixir/neuron.exs"&gt;elixir&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dev-coop/neural-net-hacking-examples/blob/master/es5/neuronet.js"&gt;javascript ES5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dev-coop/neural-net-hacking-examples/blob/master/es7/Neuron.js"&gt;javascript ES7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kblake/neural-network"&gt;ruby&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dev-coop/neural-net-hacking-examples/blob/master/python/Part%201/neural_network_with_connections.py"&gt;python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dev-coop/neural-net-hacking-examples/blob/master/python/Part%201/neural_network_with_connections_tests.py"&gt;python tests&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Stay tuned for part 2 next month! We'll be layering and maybe even training our neurons!&lt;/p&gt;
&lt;h3&gt;Bonus: Python tests&lt;/h3&gt;
&lt;p&gt;In Python a tests are many functions run one after the other checking a suite of software for errors.
These functions have simple assertions in them guaranteeing the behavior of a piece of code.
If the assertion fails, the test fails.&lt;/p&gt;
&lt;p&gt;The assertion is a condition that evaluates to a boolean, like "assert that when I add 1 + 1 I get the result 2."&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_add_one_plus_one_equals_two&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can use tests to help maintain our code and get rid of unforeseen bugs. For example, if you
are working on a website with 10,000 lines of code... how can you be sure when you touch &lt;em&gt;this thing over here&lt;/em&gt;
it doesn't break &lt;em&gt;that thing over there&lt;/em&gt;? Well! You write about twice as much code making sure the original
code does what it's supposed to by asserting exactly how it should behave.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/dev-coop/neural-net-hacking-examples/blob/master/python/Part%201/neural_network_with_connections_tests.py"&gt;Here's&lt;/a&gt; a test from the Python example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_connection_adds_to_incoming_and_outgoing_arrays&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c"&gt;# Setup our neurons and connections&lt;/span&gt;
    &lt;span class="n"&gt;neuron&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Neuron&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;neuron_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Neuron&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;neuron&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;connect_child&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;neuron_2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c"&gt;# Let&amp;#39;s make sure that there&amp;#39;s at least one connection&lt;/span&gt;
    &lt;span class="c"&gt;# from our neuron to the next neuron&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;neuron&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;connection&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;neuron&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;connection&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;neuron_2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;incoming_neurons&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

    &lt;span class="c"&gt;# And the same goes for the other neuron, make sure&lt;/span&gt;
    &lt;span class="c"&gt;# it&amp;#39;s connected to us&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;neuron_2&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;connection&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;neuron&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;connection&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;neuron&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outgoing_neurons&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If the above test passes, we can be sure our connection was made properly!&lt;/p&gt;</summary><category term="neural networks"></category><category term="machine learning"></category><category term="coding"></category></entry></feed>